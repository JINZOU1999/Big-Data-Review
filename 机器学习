# 机器学习

### 1. svm
  a. 核函数包括线性核函数和高斯核函数，在低维平面不可分时用高斯核。
	b. 损失函数为hingeloss，类似书页；lr的损失函数为loglosss
  c. svm为非参数模型；lr为参数模型；确定的函数形式会限制模型；但容易解释
  
### 2. pca 通过线性投影把高维数据映射到低维，期望在投影唯独特征方差尽可能大
	具体操作是对所有样本去中心化，计算协方差局长做特征分解，取最大n个特征值对应的向量做投影矩阵

### 3. LR公式：本质是一个线性回归，在结果映射中加了一层sigmoid function（1/1+e^(-z)）

### 4. 给我一些数据集，怎么分类
		特征维数较多-svm
		样本较大-lr
		缺失值较多-决策树

### 5. 决策树如何处理缺失值：
		a. 在选择分裂属性时
			eg使用ID3，有10个样本，第10个样本的a属性缺失。
			那么计算a属性时，用另外9个样本正常计算，然后*0.9
		b. 训练样本时
			根据a划分，但是样本a缺失。
			那么把样本分配到两个子节点，权重变为样本的比例
		c. 测试集分类时
			填充/投票决定

### 6. XGBOOST和GDBT的区别
		a. GDBT是机器学习算法，
		     XGBoost是该算法下的工程实现
		b. 在使用CART作为基分类器时，
		    XGBoost显式地加入了正则项来控制模型的复杂度，有利于防止过拟合，提高模型的泛化能力
		c. GDBT在模型训练时只使用了代价函数的一阶导数信息，
		    XGBoost对代价函数进行二阶泰勒展开，可以同时使用一阶导数和二阶导数
		d. 传统的GDBT采用CART作为基分类器
		    XGBoost支持多种类型的基分类器，比如线性分类器
		e. 传统的GDBT在每轮迭代时使用全部的数据
		    XGBoost类似于随机森林，支持对数据进行采样
		f.  传统的GDBT没有设计对缺失值进行处理
		    XGBoost能够自动学习出缺失值的处理策略

### 7. 特征选择 vs. 特征抽取
		a. 特征选择后的特征是原来特征的一个子集
		b. 特征提取后的新特征是原来特征的映射
